{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab1552c",
   "metadata": {},
   "source": [
    "# Define functions  and classes - run these before using anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_isnull(datum):\n",
    "    if pd.isnull(datum) or datum == '' or datum == 'nan':\n",
    "        return True\n",
    "    try:\n",
    "        if datum.strip(',- ') == '':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except SystemError:\n",
    "        return True\n",
    "    \n",
    "def clean_remarks_whitespace(text):\n",
    "    if my_isnull(text):\n",
    "        return text\n",
    "    else:\n",
    "        text = re.sub('\\ \\ +','  ',text)\n",
    "        text = text.replace('Conditions-not-stated', 'Conditions not stated')\n",
    "        # formatting\n",
    "        text = text.replace(\"C =\",\"C=\")\n",
    "        text = text.replace(\"C <\",\"C<\")\n",
    "        text = text.replace(\"c <\",\"c<\")\n",
    "        text = text.replace(\"I <\",\"I<\")\n",
    "        text = text.replace(\"c =\",\"c=\")\n",
    "        text = text.replace(\"C=\",\"c=\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEntryBlock(object): # A block of text partially parsed from one entry # to another\n",
    "    # Note to self: For numbers, account for repeats. There may be \"repeats\" blocks due to clerical errors\n",
    "    def __init__(self, smiles=None, number=None, original_name=None, names=None, description=None, entries=None,\n",
    "                 refs=None, nicknames=None):\n",
    "        if nicknames is None:\n",
    "            nicknames = []\n",
    "        if refs is None:\n",
    "            refs = {}\n",
    "        if entries is None:\n",
    "            entries = []\n",
    "        self.smiles = smiles\n",
    "        self.number = number\n",
    "        self.original_name = original_name\n",
    "        self.refs=refs\n",
    "        self.names = names\n",
    "        self.description = description \n",
    "        self.entries = entries\n",
    "        self.nicknames = nicknames\n",
    "        \n",
    "    def __str__(self): \n",
    "        return \"# {}: {} [{} | {}], descript. {}, refs {} | {} entries: {}\\n\".format(self.number, self.smiles, self.names, self.nicknames, self.description, self.refs, len(self.entries), self.entries)\n",
    "    \n",
    "    def export_to_dict(self):\n",
    "        entries_dict = {}\n",
    "        for ct, entry in enumerate(self.entries):\n",
    "            entries_dict[ct] = entry.to_dict()\n",
    "        out_dict = {'data': {'names':{\n",
    "                                    'SMILES': self.smiles,\n",
    "                                    'IUPAC': self.names,\n",
    "                                    'nicknames': self.nicknames\n",
    "                                     },\n",
    "                             'description':self.description,\n",
    "                             'refs': self.refs,\n",
    "                             'entries':entries_dict,}\n",
    "                   }\n",
    "        return out_dict\n",
    "    \n",
    "class DataEntry(object): # DataEntryBlocks \"hold\" multiple DataEntry objects.\n",
    "    def __init__(self, pkas=None, t=None, remarks = None, method=None, assessment=None, ref=None, subtables=None):\n",
    "        if subtables is None:\n",
    "            subtables = []\n",
    "        if ref is None:\n",
    "            ref = []\n",
    "        if assessment is None:\n",
    "            assessment = []\n",
    "        if method is None:\n",
    "            method = []\n",
    "        if t is None:\n",
    "            t = []\n",
    "        if pkas is None:\n",
    "            pkas = []\n",
    "        self.pkas = pkas\n",
    "        self.T = t\n",
    "        self.remarks = remarks\n",
    "        self.method = method\n",
    "        self.assessment = assessment\n",
    "        self.ref = ref\n",
    "        self.subtables = subtables\n",
    "\n",
    "    def __str__(self): \n",
    "        return \"pKa: {} | T: {} | Remarks: {} | Method: {} | Assessment: {} | Ref: {} \\n\".format(self.pkas, self.T, self.remarks, self.method, self.assessment, self.ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34853409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    # Turns a df into a list of block datas. Assumes that entry #s are always in the first column.\n",
    "    block = DataEntryBlock(number = None, smiles=None, original_name = None, names = None, nicknames=[], description = None, entries=[],refs={})\n",
    "    block_list = []\n",
    "    for _, row in data.iterrows():\n",
    "        for index, item in enumerate(row):\n",
    "            if index == 0:\n",
    "                entrynum = row[0]\n",
    "                if not pd.isnull(entrynum):\n",
    "                    # Try to clean up the IUPAC name\n",
    "                    iupacname = row[1].strip()\n",
    "                    iupacname = iupacname.replace(\",---\",\",\")\n",
    "                    iupacname = iupacname.replace(\",--\",\",\")\n",
    "                    iupacname = iupacname.replace(\",-\",\",\")\n",
    "                    iupacname = iupacname.replace(\".--\",\",\")\n",
    "                    iupacname = iupacname.replace(\" .-\",\",\")\n",
    "                    iupacname = iupacname.replace(\" ,\",\",\")\n",
    "                    iupacname = iupacname.replace(\", .\",\",\")\n",
    "                    iupacname = iupacname.replace(\". \",\",\")\n",
    "                    iupacname = iupacname.replace(\".-\",\"-\")\n",
    "                    iupacname = iupacname.replace(\"---\",\"-\")\n",
    "                    iupacname = iupacname.replace(\"--\",\"-\")\n",
    "                    iupacname = iupacname.replace(\"’\",\"'\")\n",
    "                    iupacname = iupacname.replace(\"”\",'\"')\n",
    "                    iupacname = iupacname.rstrip('.')\n",
    "\n",
    "                    # Greek characters\n",
    "                    iupacname = iupacname.replace(\"alpha\",\"α\")\n",
    "                    iupacname = iupacname.replace(\"beta\",\"β\")\n",
    "                    iupacname = iupacname.replace(\"gamma\",\"γ\")\n",
    "                    iupacname = iupacname.replace(\"Delta\",\"Δ\")\n",
    "                    iupacname = iupacname.replace(\"delta\",\"δ\")\n",
    "                    \n",
    "                    try:\n",
    "                        entrynum = int(entrynum)\n",
    "                    except SystemError:\n",
    "                        pass\n",
    "                    block_list.append(block) # Add previous block because it's done parsing\n",
    "                    block = DataEntryBlock(number = entrynum, smiles=None, original_name = iupacname, names = None, nicknames = [], description = None, entries=[],refs={})\n",
    "                    description = row[5]\n",
    "                    if not pd.isnull(description):\n",
    "                        block.description = \" - \" + description\n",
    "                        print(row[5])\n",
    "                else:\n",
    "                    row = row.drop(['Entry #','IUPAC'])\n",
    "                    block.entries.append(row)\n",
    "\n",
    "    block_list.append(block) # Add last block\n",
    "    block_list.remove(block_list[0])# Remove first block\n",
    "    return block_list\n",
    "\n",
    "def sort_block_list(block_list):\n",
    "    new_block_list = []\n",
    "    block_dict = {}\n",
    "    for block in block_list:\n",
    "        block_dict[str(block.number)] = block\n",
    "    sortedkeys = sorted(block_dict, key=str.lower)\n",
    "    for key in sortedkeys:\n",
    "        new_block_list.append(block_dict[key])\n",
    "    return new_block_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb0adc",
   "metadata": {},
   "source": [
    "### Load in .csvs and concatenate them into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_sample_out.csv\")\n",
    "dfs = {\"1\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort through the keys and reorder them when concatenating\n",
    "sortedkeys = sorted(dfs, key=str.lower)\n",
    "#print(sortedkeys)\n",
    "df_cat = pd.DataFrame()\n",
    "for key in sortedkeys:\n",
    "    df = dfs[key]\n",
    "    df_cat = pd.concat([df_cat, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274deecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cat.head(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57efe0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.to_csv(\"sample_all_concat.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328a093",
   "metadata": {},
   "source": [
    "### Convert and sort blocklist from .csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocklist = process_data(df_cat)\n",
    "#for block in blocklist:\n",
    "#    print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a46b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_blocklist = sort_block_list(blocklist)\n",
    "#for block in sorted_blocklist:\n",
    "#    print(block.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397df014",
   "metadata": {},
   "source": [
    "### Test if every number is represented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocknums = []\n",
    "for block in sorted_blocklist:\n",
    "    blocknums.append(block.number)\n",
    "\n",
    "allnums = list(range(2004,2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(allnums) - set(blocknums))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed75de",
   "metadata": {},
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817c33d",
   "metadata": {},
   "source": [
    "### Remove comments that only have remarks. In those cases, add remarks to block description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28942cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    for count, entry in enumerate(block.entries):\n",
    "        if my_isnull(entry['pKa']) and my_isnull(entry['pKa type']) and my_isnull(entry['T']) and my_isnull(entry['Method']) and my_isnull(entry['Ref']) and my_isnull(entry['Assessment']) and not my_isnull(entry['Remarks']):\n",
    "            if block.description is None:\n",
    "                block.description = \" - \" + entry['Remarks']\n",
    "            else:\n",
    "                block.description = block.description + \"\\n - \" + entry['Remarks']\n",
    "            block.entries.pop(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a65e2",
   "metadata": {},
   "source": [
    "### Replace many whitespaces with just two and clean up some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    for count, e in enumerate(block.entries):\n",
    "        block.entries[count]['Remarks'] = clean_remarks_whitespace(e['Remarks'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc584f",
   "metadata": {},
   "source": [
    "### Deconvolute IUPAC names into ensemble of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_PATTERN = r\"[a-z0-9\\-,A-Z\\(\\)']*-?[\\.A-MO-Z][0-9\\-a-z']+[\\[\\]\\.\\'\\\"\\’\\”HNOa-z,\\-0-9\\(\\)]*(?: acid| chloride| bromide| dichloride| fluoborate| acetate| iodide| oxime| dihydrogen phosphate| dihydrogen phosphorothioate| phosphoramidate| dioxime| thiocyanate)?\\)?\"\n",
    "WORD_PATTERN_WITH_N = r\"[a-z0-9\\-,A-Z\\(\\)']*-?[\\.A-Z][0-9\\-a-z']+[\\[\\]\\.\\'\\\"HNOa-z,\\-0-9\\(\\)]*(?: acid| chloride| bromide| dichloride| fluoborate| acetate| iodide| oxime| dihydrogen phosphate| dihydrogen phosphorothioate| phosphoramidate| dioxime| thiocyanate)?\\)?\"\n",
    "NICKNAME_PATTERN = r\"\\ \\([,-_\\w\\ 0-9']*\\(?[,-_\\w\\ 0-9]*\\)?[\\\"’,'-_\\w0-9]*\\)[\\-]?$\"\n",
    "#NICKNAME_PATTERN = r\"\\ \\([,-_\\w\\ 0-9']*\\(?[,-_\\w\\ 0-9]*\\)?[\\\",'-_\\w\\ 0-9]*\\)[\\-]?$\"\n",
    "def lowercase_func(s, ignore_n = True):\n",
    "    retstring = ''\n",
    "    for ct, c in enumerate(s):\n",
    "        if ct == 0 and (c != 'N' or ignore_n == False):\n",
    "            retstring += c.lower() \n",
    "        else:\n",
    "            retstring += c\n",
    "            \n",
    "    # special cases: \"O,O\"\n",
    "    retstring = retstring.replace(\"o,O\",\"O,O\")\n",
    "    retstring = retstring.replace(\"n,N\",\"N,N\")\n",
    "    return retstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230484f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nickname(name, debug=False):\n",
    "    # special case: name starts with parentheses\n",
    "    starts_with_parentheses = False\n",
    "    if name[0] == '(':\n",
    "        starts_with_parentheses = True\n",
    "    match = re.findall(NICKNAME_PATTERN,name)\n",
    "    if len(match) > 0:\n",
    "        nicknames = [i.strip(' ') for i in match]\n",
    "        for j in nicknames: # remove the matching nickname from the full name\n",
    "            name = name.replace(j,'').strip('()').rstrip(' ')\n",
    "#            print(\"Nickname : {}\".format(j))\n",
    "#            print(name)\n",
    "        nicknames = [fix_nickname_parentheses(i.strip('( )-')) for i in nicknames]    \n",
    "        if starts_with_parentheses:\n",
    "            name = '(' + name\n",
    "    else:\n",
    "        nicknames = []\n",
    "    if debug:\n",
    "        print(match)\n",
    "    return name, nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nickname_parentheses(nickname):\n",
    "    if nickname.count('(') > nickname.count(')'):\n",
    "        nickname = nickname + ')'\n",
    "    if nickname.count('(') < nickname.count(')'):\n",
    "        nickname = '(' + nickname\n",
    "    return nickname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out the manual names\n",
    "MANUAL_NAMES = ['Desferriferrioxamin B, N-acetyl-', 'Uridylic polynucleotide, 5-bromo-',\n",
    "               ]\n",
    "\n",
    "NICKNAMES_DICT = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ce8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_name(name, debug=False):\n",
    "    names = []\n",
    "\n",
    "    # for a few species, we need to check if its both tricky to parse the nickname AND FUll name\n",
    "    if name in NICKNAMES_DICT.keys():\n",
    "        names = NICKNAMES_DICT[name]['Names']\n",
    "        nicknames = NICKNAMES_DICT[name]['Nicknames']\n",
    "        return names, nicknames\n",
    "    \n",
    "    \n",
    "    # First, just search for nicknames. e.g. Methanal (Formaldehyde) --> Formaldehyde\n",
    "    name, nicknames = find_nickname(name)\n",
    "    if len(nicknames) > 0:\n",
    "        name, nicknames_append = find_nickname(name)\n",
    "        if nicknames_append:\n",
    "            nicknames.append(nicknames_append[0])\n",
    "            name, nicknames_append = find_nickname(name)\n",
    "            if nicknames_append:\n",
    "                nicknames.append(nicknames_append[0])\n",
    "            \n",
    "    # check if it's in the Manual Names list\n",
    "    if name in MANUAL_NAMES:\n",
    "        names = [name]\n",
    "        return names, nicknames\n",
    "\n",
    "    split_words = name.split()\n",
    "    # If we just have 1 entry, or it's a \"reference\" entry, then no need to postprocess.\n",
    "    if len(split_words) == 1 or (\"see\" in split_words and \"For\" in split_words):\n",
    "        names.append(name)\n",
    "        #print(name)\n",
    "\n",
    "        \n",
    "    # simple cases e.g. \"Sodium chloride\" shouldn't be decomposed\n",
    "    elif (len(split_words) >= 2 and (',' not in name and '-' not in name)) or (any([x in name for x in ['ketone','phosphate','acid','trioxime','phosphorodithioate',\n",
    "                                                                                                      'cation','hydrobromide','ester','dioxide', 'Phenyl',\n",
    "                                                                                                      'Benzyl', 'O-Butyl', 'O-Isopropyl', 'O-Isobutyl',\n",
    "                                                                                                      'chloride','Red','Purple','deoxystreptamine', \n",
    "                                                                                                      'Methy', 'Ethyl', 'phosphonothioate', \"Diethyl\",\n",
    "                                                                                                      'Hydroxyphenyl', 'Chlorobenzaldehyde',\n",
    "                                                                                                      'hydroperoxide','dioxime', \n",
    "                                                                                                      'isobutylphosphonothioate']]) and (', ' not in name and name[-1] != ',')):\n",
    "        names.append(name)\n",
    "#        print(name)\n",
    "    # Otherwise... Try to identify the name as a format of [Main chemical name,] + [descriptor-]\n",
    "    else:\n",
    "        if \"Naphth\" in name or \"Nonane\" in name:\n",
    "            match = re.search(WORD_PATTERN_WITH_N,name) \n",
    "            ignore_n = False\n",
    "        else:\n",
    "            match = re.search(WORD_PATTERN,name) \n",
    "            ignore_n = True\n",
    "        if match is not None: # just take the first name as this can lead to multiple matches\n",
    "            leading_name = match.group().strip(', -')\n",
    "            substring = name.replace(leading_name,'').strip(', ')\n",
    "            if len(substring.replace(' ','')) > 0: # basically if it's not just blanks\n",
    "            #print(leading_name + ' | ' + substring + ' | ' + block.name.strip())\n",
    "            \n",
    "            # Permutation one: [descriptor-main_chemical_name]\n",
    "            \n",
    "                descriptor_name = substring.rstrip('-')+lowercase_func(leading_name, ignore_n = ignore_n).rstrip(',')\n",
    "                descriptor_name_dash = substring.rstrip('-')+'-'+lowercase_func(leading_name, ignore_n = ignore_n).rstrip(',')\n",
    "                original_name = lowercase_func(leading_name, ignore_n = ignore_n).rstrip(',') + ', ' + substring.rstrip('-')+'-'\n",
    "                names.append(original_name)\n",
    "                if leading_name[0].isdigit():\n",
    "                    names.append(descriptor_name_dash)\n",
    "                else:\n",
    "                    names.append(descriptor_name)\n",
    "            else:\n",
    "                names.append(leading_name.rstrip(','))\n",
    "            if debug:\n",
    "                print('Lead: '+ leading_name)\n",
    "                print('Substring: '+substring)\n",
    "        else:\n",
    "            names.append(name)\n",
    "    \n",
    "    return names, nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    name = block.original_name\n",
    "    names, nicknames = process_name(name)\n",
    "    block.names = names\n",
    "    block.nicknames = nicknames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b099ed",
   "metadata": {},
   "source": [
    "### Copy methods/other data if no reference for entry. Also, add \"source-specific\" comments to the reference description e.g. Thermodynamic quantities are derived from the results or Other measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "OTHER_PATTERNS = r'Other [a-z]*:[ A-Za-z0-9,]*[.]?'\n",
    "OTHER_PATTERNS_INCLUSIVE = r'Other [a-z ]* in [ A-Za-z0-9,]*[.]?'\n",
    "FOR_PATTERNS = r'For [ \\[\\]a-z-A-Z0-9\\'.\\,]*see[ \\[\\]a-zA-Z0-9\\'.]*([A-Z]*[0-9]*[a-z]*[, ]?)*[.]?'\n",
    "REF_COMMENTS = ['Thermodynamic quantities for the keto and enol forms are derived from the results',\n",
    "                'Thermodynamic quantities are derived from the results for both enol and keto forms',\n",
    "                'Thermodynamic quantities are derived from the results', \n",
    "                'Thermodynamic quantities are also given',\n",
    "                'Thermodynamic data are also given',\n",
    "                'Thermodynamic quantities also given',\n",
    "                'Thermodynamic values are derived from the results'\n",
    "                'Thermodynamic quantities are derived from these results',\n",
    "                'Thermodynamic quantities are derived from the results for both enol and keto forms',\n",
    "                'Thermodynamic quantities are given',\n",
    "                'Values in other inert salt solutions are also given',\n",
    "                'Values in mixed solvents are also given',\n",
    "                'pK assignment discussed',\n",
    "                'Value in mixed solvent is also given',\n",
    "                'Thermodynamic quantities are derived from the results.2-Azapropane-1,3-diphosphonic acid, 2-ethyl- (Iminodi(methylphosphonic acid) N-ethyl-)'\n",
    "                \n",
    "                \n",
    "                \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2092cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    for count, entry in enumerate(block.entries):\n",
    "        \n",
    "        ## ONLY TAG THE REFS\n",
    "        if my_isnull(entry['Ref']):\n",
    "            block.entries[count]['Ref'] = block.entries[count-1]['Ref']\n",
    "            # Check for other missing stuff.\n",
    "            if my_isnull(entry['T']) and not my_isnull(block.entries[count-1]['T']):\n",
    "                if '(assumed)' in str(block.entries[count-1]['T']):\n",
    "                    add_string = ''\n",
    "                else:\n",
    "                    add_string = \" (assumed)\"\n",
    "                entry['T'] = str(block.entries[count-1]['T']) + add_string\n",
    "            if my_isnull(entry['Method']):\n",
    "                entry['Method'] = block.entries[count-1]['Method']\n",
    "            if my_isnull(entry['Assessment']):\n",
    "                entry['Assessment'] = block.entries[count-1]['Assessment'] \n",
    "\n",
    "        else:\n",
    "            block.refs[entry['Ref']] = []\n",
    "            \n",
    "        if not my_isnull(entry['Remarks']):\n",
    "            for ref_comment in REF_COMMENTS:\n",
    "                if ref_comment in entry['Remarks']:\n",
    "                    try:\n",
    "                        block.refs[entry['Ref']].append(ref_comment)\n",
    "                    except KeyError:\n",
    "                        block.refs[entry['Ref']] = [ref_comment]\n",
    "                    entry['Remarks'] = entry['Remarks'].replace(ref_comment,'')\n",
    "            # Other refs...\n",
    "            for pattern in [OTHER_PATTERNS, OTHER_PATTERNS_INCLUSIVE, FOR_PATTERNS]:\n",
    "                match = re.search(pattern,entry['Remarks'])\n",
    "                if match is not None:\n",
    "                    try:\n",
    "                        block.refs[entry['Ref']].append(match.group())\n",
    "                    except KeyError:\n",
    "                        block.refs[entry['Ref']] = match.group()\n",
    "                    entry['Remarks'] = entry['Remarks'].replace(match.group(),'')\n",
    "                    print(str(block.number) + \" \" + str(block.refs))            \n",
    "                    print(str(block.refs))\n",
    "                    \n",
    "        # NOW MODIFY THE ENTRIES\n",
    "        # Check for other missing stuff.\n",
    "        if my_isnull(entry['Remarks']) or entry['Remarks'] == '.' or entry['Remarks'] == '. ':\n",
    "            entry['Remarks'] = block.entries[count-1]['Remarks']\n",
    "#        else:\n",
    "#            block.refs[entry['Ref']] = []\n",
    "\n",
    "# Iterate one more time. If the thermo remarks are in the remarks, then drop it\n",
    "# If thermo is in the block.description, then move it to the block's LAST ref.\n",
    "for block in sorted_blocklist:\n",
    "    for count, entry in enumerate(block.entries):\n",
    "        if not my_isnull(entry['Remarks']):\n",
    "            for ref_comment in REF_COMMENTS:\n",
    "                # check if in remarks\n",
    "                if ref_comment in entry['Remarks']:\n",
    "                    block.entries[count]['Remarks'] = entry['Remarks'].replace(ref_comment,'')\n",
    "    # check if in description\n",
    "    if block.description:\n",
    "        for ref_comment in REF_COMMENTS:\n",
    "            if ref_comment in block.description:\n",
    "                ref_to_change = list(block.refs)[-1]\n",
    "                block.refs[ref_to_change].append(ref_comment)\n",
    "                block.description = block.description.replace(' - '+ref_comment, '')\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    # Lastly, if there's \"Other measurements\" in a ref's comments, then move it to the block\n",
    "    for ref in block.refs:\n",
    "        if len(block.refs[ref]) > 0:\n",
    "            for ct3, content in enumerate(block.refs[ref]):\n",
    "                for pattern in [OTHER_PATTERNS, OTHER_PATTERNS_INCLUSIVE, FOR_PATTERNS]:\n",
    "                    match = re.search(pattern,content)\n",
    "                    if match is not None:\n",
    "                        block.refs[ref][ct3] = content.replace(match.group(), '')\n",
    "                        if block.description is None:\n",
    "                            block.description = \" - \" + match.group()\n",
    "                        else:\n",
    "                            block.description = block.description + \"\\n - \" + match.group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38210cc7",
   "metadata": {},
   "source": [
    "### If many pKas for an entry, separate them into separate pKas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8813ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    for count, entry in enumerate(block.entries):\n",
    "        entry_copy = entry.copy()\n",
    "        try:\n",
    "            pkas = str(entry_copy['pKa']).split(',')\n",
    "            pka_types = str(entry_copy['pKa type']).split(',')\n",
    "            for ct, pka_type in enumerate(pka_types):\n",
    "                if '__0' in pka_type:\n",
    "                    pka_types[ct] = 'pKa'\n",
    "                    if len(pka_types) == 1:\n",
    "                        block.entries[count]['pKa type'] = 'pKa'\n",
    "            if len(pkas) != len(pka_types):\n",
    "                print(\"Erroneous pKa tabulation in entry # {}\".format(block.number))\n",
    "            if len(pkas) > 1:\n",
    "                entry_list = []\n",
    "                for num, pka in enumerate(pkas):\n",
    "                    duplicate = entry.copy()\n",
    "                    duplicate['pKa type'] = pka_types[num].strip()\n",
    "                    duplicate['pKa'] = pka.strip()\n",
    "                    entry_list.append(duplicate)\n",
    "                block.entries[count:count+len(pkas)-1] = entry_list\n",
    "            else:\n",
    "                pass\n",
    "        except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3be2d9",
   "metadata": {},
   "source": [
    "### Clean entry comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc761ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    for count, entry in enumerate(block.entries):\n",
    "        try:\n",
    "            remarks = str(entry['Remarks'])\n",
    "            print(remarks)\n",
    "            if remarks.strip() == '.' or remarks.strip() == '-':\n",
    "                block.entries[count]['Remarks'] = ''\n",
    "            else:\n",
    "                block.entries[count]['Remarks'] = block.entries[count]['Remarks'].replace(\"  \", \", \")\n",
    "        except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3761eb7a",
   "metadata": {},
   "source": [
    "##  DONE: Tool for adding subtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ef1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_remarks(entry_duplicate, x, running_t, running_p, running_i):\n",
    "    # Case 1: Temperature\n",
    "    if running_t:\n",
    "        entry_duplicate['T'] = x\n",
    "    elif running_p:\n",
    "        if my_isnull(entry_duplicate['Remarks']):\n",
    "            entry_duplicate['Remarks'] = 'P=' + x + ' ({})'.format(P_units)\n",
    "        else:\n",
    "            entry_duplicate['Remarks'] += '\\nP=' + x + ' ({})'.format(P_units)\n",
    "    elif running_i:\n",
    "        if my_isnull(entry_duplicate['Remarks']):\n",
    "            entry_duplicate['Remarks'] = 'I=' + str(x)\n",
    "        else:\n",
    "            entry_duplicate['Remarks'] += '\\nI=' + str(x)\n",
    "            \n",
    "    return entry_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = ['275.89', '305.32', '334.77', '364.06', '421.97', '450.21', '478.10', '505.48']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a94cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#block_numbers = []\n",
    "processed_string='already_processed'\n",
    "\n",
    "for block in sorted_blocklist:\n",
    "    for count, entry in enumerate(block.entries):\n",
    "        if not my_isnull(entry['(Subtable)']) and entry['(Subtable)'] != processed_string:\n",
    "            running_t = False\n",
    "            running_p = False\n",
    "            running_i = False\n",
    "            running_lambda = False\n",
    "\n",
    "            z = entry['(Subtable)'].replace('\\'','\"').replace('\\’','\"').replace('’','\"').replace('‘','\"')            \n",
    "            subtable_as_list = ast.literal_eval(z)\n",
    "#            z2 = json.loads(z) \n",
    "    \n",
    "            print(block.number)\n",
    "            variation_header = subtable_as_list[0]\n",
    "            print(variation_header)\n",
    "\n",
    "            # check headers\n",
    "            if 'with temperature' in variation_header:\n",
    "                running_t = True\n",
    "            elif 'pressure' in variation_header:\n",
    "                running_p = True\n",
    "                P_units = variation_header.replace('Variation with pressure','').strip('\\(\\)')\n",
    "            elif 'with I' in variation_header or 'ionic strength' in variation_header:\n",
    "                running_i = True\n",
    "            else:\n",
    "                raise ValueError(\"unknown x variable type\")\n",
    "\n",
    "            end_dict = {}\n",
    "            x_var_list = []\n",
    "            # iterate through the rows of the subtable\n",
    "            \n",
    "            for i in range(1,len(subtable_as_list)):\n",
    "                row = subtable_as_list[i]\n",
    "#                print(row)\n",
    "                if type(row) == str:\n",
    "#                    print('String Row')\n",
    "#                    if row != 'verbatim':\n",
    "                    row = row.split()\n",
    "                if type(row) == list:\n",
    "                    # This case defines the x variable, e.g. pressure or temperature                \n",
    "#                    print('List Row')\n",
    "                    x_var_list.append(row)\n",
    "                    \n",
    "                elif type(row) == dict:\n",
    "                    # These are the pkas\n",
    "#                    print('Dict Row')\n",
    "                    t = list(row.items())\n",
    "                    archetype = t[0][0]\n",
    "                    list_to_add = []\n",
    "                    for item in t:\n",
    "                        list_to_add.append(item[1])\n",
    "                    end_dict[archetype] = list_to_add\n",
    "                    \n",
    "                else:\n",
    "#                    if row != 'verbatim':\n",
    "                    raise ValueError(\"unknown row type: {}\".format(row))\n",
    "            \n",
    "            # Now, loop everything together\n",
    "            print(x_var_list)\n",
    "            print(end_dict)\n",
    "            end_list = list(end_dict.items())\n",
    "            print()\n",
    "            # y dict: {'pk1': [1, 2, 3], 'pk2': [4, 5, 6]}\n",
    "            \n",
    "            # Clean subtable tag. this needs to go first\n",
    "            block.entries[count]['(Subtable)'] = processed_string\n",
    "            # APPEND ENTRIES\n",
    "            \n",
    "            # Case A: just 1 temp list\n",
    "            if len(x_var_list) == 1:\n",
    "                for pka_list in end_list: # e.g. [(pK1, [3.1, 3.2, 3.3]), (pk2, [1.2, 2.2, 3.2])]                    \n",
    "                    for ct_x, x in enumerate(x_var_list[0]): #e.g. [5, 10, 15]\n",
    "                        entry_duplicate = copy.deepcopy(entry)\n",
    "                        entry_duplicate = assign_remarks(entry_duplicate, x, running_t, running_p, running_i)\n",
    "                        if pka_list[0] == 'lambda_0':\n",
    "                            pass\n",
    "                        else:\n",
    "                            entry_duplicate['pKa type'] = pka_list[0]\n",
    "                            entry_duplicate['pKa'] = pka_list[1][ct_x]\n",
    "                            if block.number == 2163:\n",
    "#                                print(block)\n",
    "                                entry_duplicate['Remarks'] = entry_duplicate['Remarks'].replace('lambda_0=393.23','')\n",
    "                                entry_duplicate['Remarks'] += '\\nlambda_0={}'.format(lambdas[ct_x])\n",
    "    #                        print(entry_duplicate)\n",
    "                            block.entries.append(entry_duplicate)\n",
    "            \n",
    "            # Case B: multiple temp lists. Assume temp list 1 = first pKa, temp list 2 = second pKa\n",
    "            elif len(x_var_list) > 1:\n",
    "                for ct_xlist, x_list in enumerate(x_var_list): # e.g. [[5, 10, 15], [5, 15, 20, 25]]:\n",
    "                    for ct_x, x in enumerate(x_list): #e.g. [5, 10, 15]\n",
    "                        entry_duplicate = copy.deepcopy(entry)\n",
    "                        entry_duplicate = assign_remarks(entry_duplicate, x, running_t, running_p, running_i)\n",
    "                        entry_duplicate['pKa type'] = end_list[ct_xlist][0]\n",
    "                        entry_duplicate['pKa'] = end_list[ct_xlist][1][ct_x]\n",
    "#                        print(entry_duplicate)\n",
    "                        block.entries.append(entry_duplicate)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"x_var_list = 0??\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f750b",
   "metadata": {},
   "source": [
    "# Tool for alphabetizing names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_nums = []\n",
    "names = [[],[]]\n",
    "nicknames = [[],[],[]]\n",
    "\n",
    "for block in sorted_blocklist:\n",
    "    block_nums.append(block.number)\n",
    "    for i in range(2):\n",
    "        try:\n",
    "            names[i].append(block.names[i])\n",
    "        except IndexError:\n",
    "            names[i].append('')\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            nicknames[i].append(block.nicknames[i])\n",
    "        except IndexError:\n",
    "            nicknames[i].append('')\n",
    "df = pd.DataFrame.from_dict({'Entry #': block_nums, 'Name 1':names[0], #'Name 2': names[1], \n",
    "                             'Nickname 1': nicknames[0], 'Nickname 2': nicknames[1],\n",
    "                            'Nickname 3': nicknames[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sample_names.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34e038",
   "metadata": {},
   "source": [
    "# Tool for assigning SMILES to DataEntryBlocks, need RDkit for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3383763",
   "metadata": {},
   "source": [
    "## At this point, in the workflow, I would manually create a spreadsheet (\"names/sample_names_OUT.csv\") that also has columns for OPSIN predictions. This is included in the demo for your convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1568378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import pubchempy as pcp\n",
    "import cirpy\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25998abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_pubchempy(name):\n",
    "    if pd.isnull(name):\n",
    "        return None\n",
    "\n",
    "    name = str(name)\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    name_set = set()\n",
    "    results = pcp.get_compounds(name, 'name')\n",
    "    for compound in results:\n",
    "        name_set.add(compound.isomeric_smiles)\n",
    "    \n",
    "    if len(name_set) == 1:\n",
    "        return list(name_set)[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_from_cirpy(name):\n",
    "    if pd.isnull(name):\n",
    "        return None\n",
    "\n",
    "    name = str(name)\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        smiles = cirpy.resolve(name, 'smiles', ['name_by_cir'])\n",
    "    except HTTPError:\n",
    "        smiles = ''\n",
    "    return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = None\n",
    "if 'names_df' not in locals():\n",
    "    names_df = pd.read_csv(os.path.join(os.getcwd(),\"names\",\"sample_names_OUT.csv\"))\n",
    "else:\n",
    "    print(\"names_df already loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5983ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36697d9a",
   "metadata": {},
   "source": [
    "### Create 'converged_smiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pubchem_name1' not in names_df:\n",
    "    print(\"Resolving Name 1...\")\n",
    "    names_df['pubchem_name1'] = names_df['Name 1'].apply(lambda x: get_from_pubchempy(x))\n",
    "\n",
    "if 'pubchem_name2' not in names_df:\n",
    "    print(\"Resolving Name 2...\")\n",
    "    names_df['pubchem_name2'] = names_df['Name 2'].apply(lambda x: get_from_pubchempy(x))    \n",
    "\n",
    "if 'pubchem_nickname1' not in names_df:\n",
    "    print(\"Resolving nickname 1...\")\n",
    "    names_df['pubchem_nickname1'] = names_df['Nickname 1'].apply(lambda x: get_from_pubchempy(x))    \n",
    "\n",
    "if 'pubchem_nickname2' not in names_df:\n",
    "    print(\"Resolving nickname 2...\")\n",
    "    names_df['pubchem_nickname2'] = names_df['Nickname 2'].apply(lambda x: get_from_pubchempy(x))    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cirpy_name1' not in names_df:\n",
    "    print(\"Resolving Name1...\")\n",
    "    names_df['cirpy_name1'] = names_df['Name 1'].apply(lambda x: get_from_cirpy(x))\n",
    "\n",
    "if 'cirpy_name2' not in names_df:\n",
    "    print(\"Resolving Name2...\")\n",
    "    names_df['cirpy_name2'] = names_df['Name 2'].apply(lambda x: get_from_cirpy(x))\n",
    "\n",
    "if 'cirpy_nickname1' not in names_df:\n",
    "    print(\"Resolving Nickname1...\")\n",
    "    names_df['cirpy_nickname1'] = names_df['Nickname 1'].apply(lambda x: get_from_cirpy(x))\n",
    "\n",
    "if 'cirpy_nickname2' not in names_df:\n",
    "    print(\"Resolving Nickname1...\")\n",
    "    names_df['cirpy_nickname2'] = names_df['Nickname 2'].apply(lambda x: get_from_cirpy(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e08f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.to_csv(os.path.join(os.getcwd(),\"names\",\"sample_names_OUT.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc7450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = ['OPSIN_name1', \n",
    "#           'OPSIN_name2', \n",
    "           'OPSIN_nickname1', \n",
    "           'OPSIN_nickname2',\n",
    "           'cirpy_name1',\n",
    "#           'cirpy_name2',\n",
    "           'cirpy_nickname1',\n",
    "           'cirpy_nickname2',\n",
    "           'pubchem_name1',\n",
    "#           'pubchem_name2',\n",
    "           'pubchem_nickname1',\n",
    "           'pubchem_nickname2',\n",
    "           'chemaxon_name1',\n",
    "           'chemaxon_nickname1'\n",
    "          ]\n",
    "inconsistent_ct = 0\n",
    "missing_ct = 0\n",
    "converged_ct = 0\n",
    "\n",
    "for ct, row in enumerate(names_df.iterrows()):\n",
    "    contributors = []\n",
    "    smiles_list = []\n",
    "    for header in headers:\n",
    "        smiles = row[1][header]\n",
    "        if not pd.isnull(smiles) and smiles != '':\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                smiles_list.append(Chem.MolToSmiles(mol,isomericSmiles=True))                \n",
    "                contributors.append(header)\n",
    "            except SystemError:\n",
    "                pass\n",
    "            \n",
    "    # compare with manual SMILES also\n",
    "    smiles_set = set(smiles_list)\n",
    "    \n",
    "\n",
    "    if len(smiles_set) > 1:\n",
    "        # Algorithm: Let's say there are only 2 entries. And one of them is the isomeric version.\n",
    "        # In that case, just append the isomeric version.\n",
    "        if len(smiles_set) == 2:\n",
    "            smiles_set_unwrapped = list(smiles_set)\n",
    "            mol0 = Chem.MolFromSmiles(smiles_set_unwrapped[0])\n",
    "            mol1 = Chem.MolFromSmiles(smiles_set_unwrapped[1])\n",
    "            if Chem.MolToSmiles(mol0, isomericSmiles=False) == Chem.MolToSmiles(mol1):\n",
    "                converged_smiles = smiles_set_unwrapped[0]\n",
    "                print(\"Isomeric: {} for molecule {}\".format(converged_smiles, row[1]['Name 1']))\n",
    "                names_df.loc[ct, 'type'] = 'isomeric'\n",
    "                converged_ct += 1\n",
    "            elif Chem.MolToSmiles(mol1, isomericSmiles=False) == Chem.MolToSmiles(mol0):\n",
    "                converged_smiles = smiles_set_unwrapped[1]\n",
    "                print(\"Isomeric: {} for molecule {}\".format(converged_smiles, row[1]['Name 1']))\n",
    "                names_df.loc[ct, 'type'] = 'isomeric'\n",
    "                converged_ct += 1\n",
    "            else:\n",
    "                converged_smiles = ''\n",
    "                inconsistent_ct += 1\n",
    "                print(\"error with SMILES set: {} | molecule {}\".format(smiles_set, row[1]['Name 1']))\n",
    "                names_df.loc[ct, 'type'] = 'inconsistent'\n",
    "        else:        \n",
    "            converged_smiles = ''\n",
    "            names_df.loc[ct, 'type'] = 'inconsistent'\n",
    "            print(\"error with SMILES set: {} | molecule {}\".format(smiles_set, row[1]['Name 1']))\n",
    "            inconsistent_ct += 1\n",
    "    elif len(smiles_set) == 0:\n",
    "        names_df.loc[ct, 'type'] = 'missing'\n",
    "        converged_smiles = ''\n",
    "        missing_ct += 1\n",
    "    else:\n",
    "        names_df.loc[ct, 'type'] = 'converged'\n",
    "        converged_smiles = smiles_set.pop()\n",
    "        converged_ct += 1\n",
    "    names_df.loc[ct,'converged_smiles'] = converged_smiles\n",
    "    if len(converged_smiles) > 0:\n",
    "        names_df.loc[ct, 'contributors'] = str(contributors)\n",
    "    else:\n",
    "        names_df.loc[ct, 'contributors'] = ''\n",
    "    names_df.loc[ct, 'num_contributors'] = int(len(contributors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inconsistent_ct)\n",
    "print(missing_ct)\n",
    "print(converged_ct)\n",
    "print(\"Inconsistent total: {}\".format(inconsistent_ct / (converged_ct + missing_ct + inconsistent_ct)))\n",
    "print(\"Missing total: {}\".format(missing_ct / (converged_ct + missing_ct + inconsistent_ct)))\n",
    "print(\"Converged total: {}\".format(converged_ct / (converged_ct + missing_ct + inconsistent_ct)))\n",
    "names_df.head(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83764c6c",
   "metadata": {},
   "source": [
    "### print out csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_dict = {}\n",
    "for row in names_df.iterrows():\n",
    "    smiles = row[1]['converged_smiles']\n",
    "    contributors = row[1]['contributors']\n",
    "    num_contributors = row[1]['num_contributors']\n",
    "    if not my_isnull(smiles):\n",
    "        entry_dict[row[1]['Entry #']] = [smiles, contributors, num_contributors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10984ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0132bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    try:\n",
    "        block.smiles = entry_dict[block.number][0]\n",
    "        block.contributors = entry_dict[block.number][1]\n",
    "        block.num_contributors = entry_dict[block.number][2]\n",
    "        print(block.smiles)\n",
    "    except KeyError:\n",
    "        block.smiles = None\n",
    "        block.contributors = None\n",
    "        block.num_contributors = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5feec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in sorted_blocklist:\n",
    "    print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7fe818",
   "metadata": {},
   "source": [
    "### Convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da14f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_remarks_wrapper(refs, entryref):\n",
    "    if my_isnull(entryref):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return refs[entryref]\n",
    "\n",
    "def unpack(strlist):\n",
    "    # Unpacks a list into a string separated by semicolons\n",
    "    if type(strlist) == list:\n",
    "        return_string = ''\n",
    "        for ct,string in enumerate(strlist):\n",
    "            if ct == 0:\n",
    "                return_string = string\n",
    "            else:\n",
    "                return_string += \"; \" + string\n",
    "        return return_string\n",
    "    else:\n",
    "        return strlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['entry_#','SMILES','pka_type','pka_value','T','remarks','method','assessment','ref','ref_remarks','entry_remarks','original_IUPAC_names'])\n",
    "\n",
    "for block in sorted_blocklist:\n",
    "    for entry in block.entries:\n",
    "        df = df.append({\n",
    "            \"entry_#\": block.number,\n",
    "            \"SMILES\": block.smiles,\n",
    "            \"pka_type\": entry[0],\n",
    "            \"pka_value\": entry[1],\n",
    "            \"T\": entry[2],\n",
    "            \"remarks\": entry[3],\n",
    "            \"method\": entry[4],\n",
    "            \"assessment\": entry[5],\n",
    "            \"ref\": entry[6],\n",
    "            \"ref_remarks\": unpack(ref_remarks_wrapper(block.refs, entry[6])),\n",
    "            \"entry_remarks\": block.description,\n",
    "#            \"original_IUPAC_names\": unpack(block.names),\n",
    "            \"original_IUPAC_names\": block.names[0],\n",
    "            \"original_IUPAC_nicknames\": unpack(block.nicknames),\n",
    "            \"name_contributors\": block.contributors,\n",
    "            \"num_name_contributors\": block.num_contributors,\n",
    "        },ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c649f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e86ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sample_done.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
